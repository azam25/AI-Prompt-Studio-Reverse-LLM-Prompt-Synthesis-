"""Evaluation LLM service for analyzing prompt outputs and identifying issues."""
from typing import List, Dict, Tuple
from openai import OpenAI
import json
import re

from app.config import settings
from app.models.prompt import (
    ExpectedOutput, 
    EvaluationResult, 
    RootCauseCategory,
    ChatMLPrompt,
    ChatMLMessage,
    ChatMLRole
)
from app.models.evaluation import ContextQualityAssessment


class EvaluationLLM:
    """Evaluation LLM for assessing prompt quality and identifying improvements."""
    
    def __init__(self):
        self.client = None
        self._current_api_key = None
        self._current_base_url = None
    
    def _ensure_client(self):
        """Ensure OpenAI client is initialized and up-to-date."""
        # Reinitialize if settings have changed
        if (settings.openai_api_key and 
            (self._current_api_key != settings.openai_api_key or 
             self._current_base_url != settings.openai_base_url)):
            self.client = OpenAI(
                api_key=settings.openai_api_key,
                base_url=settings.openai_base_url
            )
            self._current_api_key = settings.openai_api_key
            self._current_base_url = settings.openai_base_url
        
        if not self.client:
            raise ValueError("OpenAI client not initialized. Please configure API key.")
    
    def generate_output(self, prompt: ChatMLPrompt) -> str:
        """
        Generate output using the given prompt.
        
        Args:
            prompt: ChatML format prompt
            
        Returns:
            Generated output string
        """
        self._ensure_client()
        
        messages = [{"role": m.role.value, "content": m.content} for m in prompt.messages]
        
        response = self.client.chat.completions.create(
            model=prompt.model or settings.llm_model,
            messages=messages,
            temperature=prompt.temperature if prompt.temperature is not None else settings.temperature,
            max_tokens=prompt.max_tokens or settings.max_tokens
        )
        
        return response.choices[0].message.content
    
    def evaluate(
        self,
        generated_output: str,
        expected_output: ExpectedOutput,
        retrieved_contexts: List[str],
        iteration: int
    ) -> EvaluationResult:
        """
        Evaluate generated output against expected output.
        
        Args:
            generated_output: The output generated by the prompt
            expected_output: User's expected output definition
            retrieved_contexts: Contexts that were retrieved
            iteration: Current iteration number
            
        Returns:
            EvaluationResult with analysis and suggestions
        """
        self._ensure_client()
        
        system_prompt = """You are an expert evaluator for RAG system outputs.
Your task is to compare a generated output with an expected output template and:
1. Calculate a match score (0.0 to 1.0)
2. Identify root causes of any mismatches
3. Suggest improvements

Root cause categories:
- context_missing: The retrieved context doesn't contain the needed information
- terminology_mismatch: The query uses different terms than the document
- structure_mismatch: The output structure doesn't match expected format
- ambiguity: The query or expected output is ambiguous
- retrieval_quality: The retrieval didn't return the most relevant chunks

Respond in JSON format:
{
    "match_score": 0.0-1.0,
    "root_causes": ["category1", "category2"],
    "improvement_suggestions": ["suggestion1", "suggestion2"],
    "is_successful": true/false,
    "analysis": "brief analysis"
}"""

        user_prompt = f"""Expected Output Template:
{expected_output.template}

Generated Output:
{generated_output}

Retrieved Context Snippets:
{chr(10).join(f'- {ctx[:200]}...' for ctx in retrieved_contexts[:3])}

Evaluate the match and identify issues. Consider:
1. Are the placeholder types correctly filled?
2. Is the structure correct?
3. Is any information missing that should be in the context?
4. Are there terminology mismatches?"""

        response = self.client.chat.completions.create(
            model=settings.llm_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )
        
        # Parse the response
        try:
            result_text = response.choices[0].message.content
            # Extract JSON from response
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                result_data = json.loads(json_match.group())
            else:
                result_data = {}
        except json.JSONDecodeError:
            result_data = {}
        
        # Map string root causes to enum
        root_causes = []
        for rc in result_data.get('root_causes', []):
            try:
                root_causes.append(RootCauseCategory(rc))
            except ValueError:
                pass
        
        return EvaluationResult(
            iteration=iteration,
            generated_output=generated_output,
            match_score=result_data.get('match_score', 0.5),
            root_causes=root_causes,
            improvement_suggestions=result_data.get('improvement_suggestions', []),
            is_successful=result_data.get('is_successful', False)
        )
    
    def analyze_context_quality(
        self,
        expected_output: ExpectedOutput,
        retrieved_contexts: List[str]
    ) -> ContextQualityAssessment:
        """
        Analyze the quality of retrieved context for the expected output.
        
        Args:
            expected_output: User's expected output definition
            retrieved_contexts: Retrieved context chunks
            
        Returns:
            ContextQualityAssessment with detailed analysis
        """
        self._ensure_client()
        
        system_prompt = """Analyze the retrieved context chunks for a RAG query.
Determine if they contain the information needed to fill the expected output template.

Respond in JSON:
{
    "relevant_chunks": number of chunks with relevant info,
    "relevance_score": 0.0-1.0,
    "missing_information": ["info1", "info2"],
    "terminology_gaps": ["gap1", "gap2"]
}"""

        user_prompt = f"""Expected Output Template:
{expected_output.template}

Retrieved Contexts:
{chr(10).join(f'{i+1}. {ctx}' for i, ctx in enumerate(retrieved_contexts))}

Analyze each context for relevance to the template placeholders."""

        response = self.client.chat.completions.create(
            model=settings.llm_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=800
        )
        
        try:
            result_text = response.choices[0].message.content
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                result_data = json.loads(json_match.group())
            else:
                result_data = {}
        except json.JSONDecodeError:
            result_data = {}
        
        return ContextQualityAssessment(
            total_chunks=len(retrieved_contexts),
            relevant_chunks=result_data.get('relevant_chunks', 0),
            relevance_score=result_data.get('relevance_score', 0.5),
            missing_information=result_data.get('missing_information', []),
            terminology_gaps=result_data.get('terminology_gaps', [])
        )


# Singleton instance
evaluation_llm = EvaluationLLM()


def get_evaluation_llm() -> EvaluationLLM:
    """Get the evaluation LLM instance."""
    return evaluation_llm
